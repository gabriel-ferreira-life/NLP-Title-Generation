{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11080504,"sourceType":"datasetVersion","datasetId":6906093},{"sourceId":11080527,"sourceType":"datasetVersion","datasetId":6906111},{"sourceId":228488690,"sourceType":"kernelVersion"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":null,"end_time":null,"environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-03-19T15:53:41.809092","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"f80735cc","cell_type":"markdown","source":"## Libraries","metadata":{"papermill":{"duration":0.007206,"end_time":"2025-03-19T15:53:44.503482","exception":false,"start_time":"2025-03-19T15:53:44.496276","status":"completed"},"tags":[]}},{"id":"798a7c71-6b9e-4106-985d-b9a211360663","cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U accelerate\n%pip install -U transformers accelerate\n%pip install -U peft\n%pip install -U trl\n%pip install GPUtil\n%pip install evaluate\n%pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T23:25:15.843365Z","iopub.execute_input":"2025-04-17T23:25:15.843554Z","iopub.status.idle":"2025-04-17T23:27:14.263495Z","shell.execute_reply.started":"2025-04-17T23:25:15.843537Z","shell.execute_reply":"2025-04-17T23:27:14.262649Z"}},"outputs":[],"execution_count":1},{"id":"e91a2327","cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport shutil\nfrom IPython.display import FileLink, display\n\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport bitsandbytes as bnb\nfrom evaluate import load as load_metric\nfrom datasets import DatasetDict, Dataset, load_dataset\nfrom peft import LoraConfig, PeftConfig, get_peft_model, TaskType\nfrom trl import setup_chat_format, SFTTrainer\nimport transformers\nfrom transformers import (AutoTokenizer, \n                          AutoModelForCausalLM,\n                          AutoModelForSequenceClassification,\n                          BitsAndBytesConfig,\n                          Trainer,\n                          TrainingArguments, \n                          pipeline, \n                          logging,\n                          DataCollatorWithPadding)\n\nfrom huggingface_hub import login as hf_login\nfrom kaggle_secrets import UserSecretsClient\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2025-04-18T05:33:48.508841Z","iopub.execute_input":"2025-04-18T05:33:48.509030Z","iopub.status.idle":"2025-04-18T05:33:48.514695Z"},"papermill":{"duration":null,"end_time":null,"exception":false,"start_time":"2025-03-19T15:53:50.024834","status":"running"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"f20a880a","cell_type":"markdown","source":"## Config","metadata":{"papermill":{"duration":0.006065,"end_time":"2025-03-19T15:53:45.283077","exception":false,"start_time":"2025-03-19T15:53:45.277012","status":"completed"},"tags":[]}},{"id":"1ced70ad","cell_type":"code","source":"# Load the config file\nwith open('/kaggle/input/config/config.json', 'r') as f:\n    config = json.load(f)\n\nfile_path = config[\"data_loc\"]\n\ndata_loc = \"/kaggle/input/\"\nmodel_path = \"meta-llama/Llama-3.2-3B-Instruct\"\noutput_dir=\"llama-3.2-fine-tuned-model\"\nEXPERIMENTS_FILE = \"experiment_results.csv\"\n\n# Tokens\nuser_secrets = UserSecretsClient()\naccess_token = user_secrets.get_secret(\"hf_read_token_access\")\nwrite_access_token = user_secrets.get_secret(\"hf_write_token_access\")\nwb_access_token = user_secrets.get_secret(\"wanda_token\")\nhf_login(write_access_token)\n\n\nwandb.login(key=wb_access_token)\nrun = wandb.init(\n    project='Fine-tune Llama-3.2-3B-Instruct for QTL Title Generation', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2025-04-18T00:23:09.258367Z","iopub.execute_input":"2025-04-18T00:23:09.258879Z","iopub.status.idle":"2025-04-18T00:23:16.246337Z","shell.execute_reply.started":"2025-04-18T00:23:09.258858Z","shell.execute_reply":"2025-04-18T00:23:16.245551Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabriel-ferreira\u001b[0m (\u001b[33mgabriel-ferreira-iowa-state-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250418_002309-7p3bhtr5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/gabriel-ferreira-iowa-state-university/Fine-tune%20Llama-3.2-3B-Instruct%20for%20QTL%20Title%20Generation/runs/7p3bhtr5?apiKey=e9ae21b07ffe9cae402250cb116b0d8a66e9f8c6' target=\"_blank\">magic-capybara-1</a></strong> to <a href='https://wandb.ai/gabriel-ferreira-iowa-state-university/Fine-tune%20Llama-3.2-3B-Instruct%20for%20QTL%20Title%20Generation?apiKey=e9ae21b07ffe9cae402250cb116b0d8a66e9f8c6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/gabriel-ferreira-iowa-state-university/Fine-tune%20Llama-3.2-3B-Instruct%20for%20QTL%20Title%20Generation?apiKey=e9ae21b07ffe9cae402250cb116b0d8a66e9f8c6' target=\"_blank\">https://wandb.ai/gabriel-ferreira-iowa-state-university/Fine-tune%20Llama-3.2-3B-Instruct%20for%20QTL%20Title%20Generation?apiKey=e9ae21b07ffe9cae402250cb116b0d8a66e9f8c6</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/gabriel-ferreira-iowa-state-university/Fine-tune%20Llama-3.2-3B-Instruct%20for%20QTL%20Title%20Generation/runs/7p3bhtr5?apiKey=e9ae21b07ffe9cae402250cb116b0d8a66e9f8c6' target=\"_blank\">https://wandb.ai/gabriel-ferreira-iowa-state-university/Fine-tune%20Llama-3.2-3B-Instruct%20for%20QTL%20Title%20Generation/runs/7p3bhtr5?apiKey=e9ae21b07ffe9cae402250cb116b0d8a66e9f8c6</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Do NOT share these links with anyone. They can be used to claim your runs."},"metadata":{}}],"execution_count":27},{"id":"2d5af6d1-f398-4db8-a04b-7061b9ddd971","cell_type":"code","source":"# Define reusable template\nprompt_template = \"\"\"\nYou are an expert at writing concise and informative research paper titles based on abstracts.\n\nGiven the abstract below, generate only a clear, accurate, and concise title that best reflects the core idea of the abstract. I only need the title from you with no more than 45 words.\n\nAbstract: {abstract}\n\nTitle: {title}\n\"\"\".strip()\n\n# Generation for training (with known title)\ndef generate_prompt(data_point):\n    return prompt_template.format(abstract=data_point[\"Abstract\"], title=data_point[\"Title\"])\n\n# Generation for testing (no title provided)\ndef generate_test_prompt(data_point):\n    return prompt_template.format(abstract=data_point[\"Abstract\"], title=\"\")\n\n# Define text preprocessing\ndef preprocess_function(example):\n    tokens = tokenizer(example['text'], truncation=True, padding='max_length', max_length=256)\n    return tokens\n    \ndef predict(test_data, model, tokenizer, max_new_tokens=45, temperature=0.1, batch_size=8):\n    # Convert to Hugging Face Dataset if it's a DataFrame\n    if isinstance(test_data, pd.DataFrame):\n        test_data = Dataset.from_pandas(test_data)\n\n    prompts = test_data\n\n    pipe = pipeline(\n        task=\"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        device_map=\"auto\"\n    )\n\n    y_pred = []\n    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating Titles\"):\n        batch = prompts[i:i + batch_size]\n        results = pipe(batch)\n        for r in results:\n            text = r[0][\"generated_text\"] if isinstance(r, list) else r[\"generated_text\"]\n            title = text.split(\"Title: \")[-1].split(\"\\n\")[0].strip()\n            y_pred.append(title)\n\n\n    return y_pred\n\ndef evaluate(y_true, y_pred):\n    from evaluate import load as load_metric\n\n    bleu = load_metric(\"bleu\")\n    rouge = load_metric(\"rouge\")\n\n    bleu_eval = bleu.compute(predictions=y_pred, references=y_true)\n    rouge_eval = rouge.compute(predictions=y_pred, references=y_true)\n\n    required_metrics = {\n        \"BLEU\": round(bleu_eval[\"bleu\"], 4),\n        \"ROUGE-2\": round(rouge_eval[\"rouge2\"], 4),\n        \"ROUGE-L\": round(rouge_eval[\"rougeL\"], 4)\n    }\n\n    return required_metrics\n\ndef format_experiment_metrics(experiment_metadata, metrics):\n    return pd.DataFrame([experiment_metadata | metrics])\n\ndef save_experiment(experiment_metadata, metrics, path=EXPERIMENTS_FILE, mode=\"append\"):\n    new_record = format_experiment_metrics(experiment_metadata, metrics)\n\n    if mode == \"visualize\":\n        pass\n    elif mode == \"overwrite\" or not os.path.exists(path):\n        new_record.to_csv(path, index=False)\n        print(f\"Experiment saved to {path}\")\n    elif mode == \"append\":\n        existing = pd.read_csv(path)\n        new_record = pd.concat([existing, new_record], ignore_index=True)\n        new_record.to_csv(path, index=False)\n        print(f\"Experiment saved to {path}\")\n    else:\n        raise ValueError(\"Mode must be 'append' or 'overwrite'\")    \n\n    return new_record\n    \ndef find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:  # needed for 16 bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T23:27:44.428513Z","iopub.execute_input":"2025-04-17T23:27:44.428698Z","iopub.status.idle":"2025-04-17T23:27:44.441339Z","shell.execute_reply.started":"2025-04-17T23:27:44.428682Z","shell.execute_reply":"2025-04-17T23:27:44.440544Z"}},"outputs":[],"execution_count":4},{"id":"47ac11e1","cell_type":"markdown","source":"## Preparing Dataset","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"id":"3316c7e5","cell_type":"code","source":"# Data Location Path\ndata_loc = \"/kaggle/input/579nlp-project2\"\n\n# Test Set\nfile_name = \"test_unlabeled.tsv\"\nfinal_path = os.path.join(data_loc, file_name) \ndf_test = pd.read_csv(final_path, sep='\\t').drop(columns=['PMID'])\nprint(f\"The test set has {df_test.shape[0]} observations and {df_test.shape[1]} columns.\\n\")\n\n# Train Set\nfile_name = \"QTL_text.json\"\nfinal_path = os.path.join(data_loc, file_name) \ndf_train = pd.read_json(final_path)\ndf_train = df_train.drop(columns=['Journal', 'PMID'])\n# df_train = df_train[0:100]\nprint(f\"The train set has {df_train.shape[0]} observations and {df_train.shape[1]} columns.\\n\")\n\n# Prepare Input Text\ndf_train['input_text'] = df_train.apply(lambda x: generate_prompt(x), axis=1).tolist()\ndf_test['input_text'] = df_test.apply(lambda x: generate_test_prompt(x), axis=1).tolist()\n\n# Split train and validation\nX_train, X_val = train_test_split(df_train, test_size=.2, random_state=42)\n\n# Set test\nX_test = pd.DataFrame(df_test['input_text'], columns=[\"input_text\"])\ny_test = df_test['Title']","metadata":{"execution":{"iopub.status.busy":"2025-04-18T00:33:48.062430Z","iopub.execute_input":"2025-04-18T00:33:48.062711Z","iopub.status.idle":"2025-04-18T00:33:48.300236Z","shell.execute_reply.started":"2025-04-18T00:33:48.062692Z","shell.execute_reply":"2025-04-18T00:33:48.299470Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"The test set has 1097 observations and 3 columns.\n\nThe train set has 11278 observations and 3 columns.\n\n","output_type":"stream"}],"execution_count":37},{"id":"b00a860c","cell_type":"code","source":"# Training Data\ntrain_data = {\"text\": X_train['input_text']}\ntrain_dataset = Dataset.from_dict(train_data)\n\n# Validation Data\nval_data = {\"text\": X_val['input_text']}\nval_dataset = Dataset.from_dict(val_data)\n\n# Test Data\ntest_data = {\"text\": X_test['input_text'], \"titles\": y_test.astype(str).tolist()}\ntest_dataset = Dataset.from_dict(test_data)\n\n# Dataset Dictionary \ndataset_dict = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": val_dataset,\n    \"test\": test_dataset\n})\n\ndataset_dict","metadata":{"execution":{"iopub.status.busy":"2025-04-18T00:33:51.031900Z","iopub.execute_input":"2025-04-18T00:33:51.032608Z","iopub.status.idle":"2025-04-18T00:33:51.160982Z","shell.execute_reply.started":"2025-04-18T00:33:51.032583Z","shell.execute_reply":"2025-04-18T00:33:51.160397Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 9022\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 2256\n    })\n    test: Dataset({\n        features: ['text', 'titles'],\n        num_rows: 1097\n    })\n})"},"metadata":{}}],"execution_count":38},{"id":"f7a0ca48-6658-425a-b30f-4780f3369afa","cell_type":"code","source":"print(dataset_dict['train']['text'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T23:27:45.313923Z","iopub.execute_input":"2025-04-17T23:27:45.314142Z","iopub.status.idle":"2025-04-17T23:27:45.340639Z","shell.execute_reply.started":"2025-04-17T23:27:45.314104Z","shell.execute_reply":"2025-04-17T23:27:45.340066Z"}},"outputs":[{"name":"stdout","text":"You are an expert at writing concise and informative research paper titles based on abstracts.\n\nGiven the abstract below, generate only a clear, accurate, and concise title that best reflects the core idea of the abstract. I only need the title from you with no more than 45 words.\n\nAbstract: Reprogramming of adipocyte function in obesity is implicated in metabolic disorders like type 2 diabetes. Here, we used the pig, an animal model sharing many physiological and pathophysiological similarities with humans, to perform in-depth epigenomic and transcriptomic characterization of pure adipocyte fractions. Using a combined DNA methylation capture sequencing and Reduced Representation bisulfite sequencing (RRBS) strategy in 11 lean and 12 obese pigs, we identified in 3529 differentially methylated regions (DMRs) located at close proximity to-, or within genes in the adipocytes. By sequencing of the transcriptome from the same fraction of isolated adipocytes, we identified 276 differentially expressed transcripts with at least one or more DMR. These transcripts were over-represented in gene pathways related to MAPK, metabolic and insulin signaling. Using a candidate gene approach, we further characterized 13 genes potentially regulated by DNA methylation and identified putative transcription factor binding sites that could be affected by the differential methylation in obesity. Our data constitute a valuable resource for further investigations aiming to delineate the epigenetic etiology of metabolic disorders.\n\nTitle: Epigenetic and Transcriptomic Characterization of Pure Adipocyte Fractions From Obese Pigs Identifies Candidate Pathways Controlling Metabolism.\n","output_type":"stream"}],"execution_count":7},{"id":"e9c020d6-1dfe-44f0-9c17-ea4eb65ad1f6","cell_type":"code","source":"print(dataset_dict['test']['text'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T23:27:45.341351Z","iopub.execute_input":"2025-04-17T23:27:45.341647Z","iopub.status.idle":"2025-04-17T23:27:45.360081Z","shell.execute_reply.started":"2025-04-17T23:27:45.341615Z","shell.execute_reply":"2025-04-17T23:27:45.359392Z"}},"outputs":[{"name":"stdout","text":"You are an expert at writing concise and informative research paper titles based on abstracts.\n\nGiven the abstract below, generate only a clear, accurate, and concise title that best reflects the core idea of the abstract. I only need the title from you with no more than 45 words.\n\nAbstract: Porcine circovirus type 3 (PCV3) is regularly reported in association with various clinical presentations, including porcine dermatitis and nephropathy syndrome (PDNS)-like lesions, respiratory signs, congenital tremor, and reproductive disorders. To investigate the epidemiology of PCV3 in a boar stud, we analysed fresh boar semen and matching sera from 181 boars from a German stud  supplying semen for artificial insemination (AI) to approximately 740 breeder farms for PCV3 DNA. PCV3 DNA was detected in 1.7% semen samples and 24.3% sera. Spearman rho correlation demonstrated a significant positive correlation between  boar age and quantitative DNA (by PCR quantification cycles [Cq] values) in serum samples (r = 0.636; P < 0.001). Sera from boars up to 12 months of age had higher viral loads (P < 0.001) and were PCV3-positive significantly more often (P < 0.01) than older boars. Detection of PCV3 DNA was not associated with breed (P> 0.05). PCV3 DNA was detected sporadically in fresh boar semen. Based on the assumption that processing fresh semen reduces viral load in semen used for AI, it is likely that the risk of sexual transmission of PCV3 during AI in is low. However, young boars may contribute to the maintenance of PCV3 infection in boar  studs.\n\nTitle: \n","output_type":"stream"}],"execution_count":8},{"id":"5f72a3f8-b64a-419d-a472-e7368a45eaf6","cell_type":"markdown","source":"## Load Model","metadata":{}},{"id":"a66c7496-b781-46d2-ac02-dfc25de8c64d","cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    torch_dtype=\"float16\",\n    quantization_config=bnb_config, \n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    token=access_token,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:42:13.717262Z","iopub.execute_input":"2025-04-18T00:42:13.717832Z","iopub.status.idle":"2025-04-18T00:42:23.781531Z","shell.execute_reply.started":"2025-04-18T00:42:13.717807Z","shell.execute_reply":"2025-04-18T00:42:23.780769Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6bb1a042fb34f7f8dd9bda56858019a"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":50},{"id":"f42db365-fd68-4f7e-b570-7bb20eca3351","cell_type":"code","source":"# Preprocess all datasets\ntokenized_data = dataset_dict.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:33:55.662999Z","iopub.execute_input":"2025-04-18T00:33:55.663617Z","iopub.status.idle":"2025-04-18T00:34:10.934557Z","shell.execute_reply.started":"2025-04-18T00:33:55.663590Z","shell.execute_reply":"2025-04-18T00:34:10.933867Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9022 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4687673a8e2e42529eae5154b605ce86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2256 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adddc344b1ee4779830eedbf9935af67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1097 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dd2f3eae2704377b4bdbe0cc8eb6ded"}},"metadata":{}}],"execution_count":39},{"id":"7ed3c83a-97ed-4a39-aca2-aa2af3e8fd01","cell_type":"markdown","source":"## Model Demonstration - Generation","metadata":{}},{"id":"2ae34ce1-a94d-408b-95bb-358d1da8edf5","cell_type":"code","source":"# Sample a prompt\nn=4\nprompt = X_test['input_text'][n]\n\n# print(prompt)\noutputs = pipe(prompt, max_new_tokens=45, do_sample=True, pad_token_id=tokenizer.eos_token_id)\nprint(outputs[0][\"generated_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T23:28:27.133582Z","iopub.execute_input":"2025-04-17T23:28:27.134240Z","iopub.status.idle":"2025-04-17T23:28:30.077778Z","shell.execute_reply.started":"2025-04-17T23:28:27.134223Z","shell.execute_reply":"2025-04-17T23:28:30.077114Z"}},"outputs":[{"name":"stdout","text":"You are an expert at writing concise and informative research paper titles based on abstracts.\n\nGiven the abstract below, generate only a clear, accurate, and concise title that best reflects the core idea of the abstract. I only need the title from you with no more than 45 words.\n\nAbstract: BACKGROUND: Acute or chronic irreversible respiratory failure may occur in patients undergoing pneumonectomy. Aim of this study was to determine transcriptome expression changes after experimental pneumonectomy in swine model. Experimental left pneumonectomy was performed in five pigs under general anaesthesia. Both the resected and the remaining lung, after 60 post-operative completely uneventful days, underwent genome-wide bulk RNA-Sequencing (RNA-Seq). RESULTS: Histological analysis showed dilation of air spaces and rupture of interalveolar septa. In addition, mild inflammation, no fibrosis, radial stretch  of the bronchus, strong enlargement of airspaces and thinning of the blood supply were observed. Bioinformatic analyses of bulk RNA-Seq data identified 553 Differentially Expressed Genes (DEGs) at adjusted P-value below 0.001, between pre- and post-pneumonectomy. The top 10 up-regulated DEGs were Edn1, Areg, Havcr2, Gadd45g, Depp1, Cldn4, Atf3, Myc, Gadd45b, Socs3; the top 10 down-regulated DEGs were Obscn, Cdkn2b, ENSSSCG00000015738, Prrt2, Amer1, Flrt3,  Efnb2, Tox3, Znf793, Znf365. Leveraging digital cytometry tools, no difference in cellular abundance was found between the two experimental groups, while the analysis of cell type-specific gene expression patterns highlighted a striking predominance of macrophage-specific genes among the DEGs. DAVID-based gene ontology analysis showed a significant enrichment of \"Extrinsic apoptotic signaling pathway\" (FDR q = 7.60 × 10- 3) and \"Response to insulin\" (FDR q = 7.60 × 10- 3) genes, along with an enrichment of genes involved as \"Negative  regulators of DDX58/IFIH1 signaling\" (FDR q = 7.50 × 10- 4) found by querying the REACTOME pathway database. Gene network analyses indicated a general dysregulation of gene inter-connections. CONCLUSION: This translational genomics study highlighted the existence both of individual genes, mostly dysregulated in certain cellular populations (e.g., macrophages), and gene-networks involved in pulmonary reaction after left pneumonectomy. Their involvement in lung homeostasis is largely supported by previous studies, carried out both in humans and in other animal models (under homeostatic or disease-related conditions), that adopted candidate-gene approaches. Overall, the present findings represent a preliminary assessment for  future, more focused, studies on compensatory lung adaptation, pulmonary regeneration and functional reload.\n\nTitle:  Transcriptome expression changes after experimental pneumonectomy in a swine model: A study on the pulmonary response to lung resection. \n\nPlease let me know if you can make any adjustments or changes to the title to better reflect\n","output_type":"stream"}],"execution_count":11},{"id":"91d23952-1c3b-4a22-8035-54d4aa0764e3","cell_type":"code","source":"pred = [outputs[0][\"generated_text\"].split(\"Title: \")[-1].split(\" \\n\")[0]]\npred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T23:28:30.078457Z","iopub.execute_input":"2025-04-17T23:28:30.078642Z","iopub.status.idle":"2025-04-17T23:28:30.083778Z","shell.execute_reply.started":"2025-04-17T23:28:30.078627Z","shell.execute_reply":"2025-04-17T23:28:30.083178Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[' Transcriptome expression changes after experimental pneumonectomy in a swine model: A study on the pulmonary response to lung resection.']"},"metadata":{}}],"execution_count":12},{"id":"65f9f049-b75e-4696-83e9-399fb230c793","cell_type":"code","source":"reference =[ y_test[n]]\nreference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T23:28:30.084469Z","iopub.execute_input":"2025-04-17T23:28:30.084702Z","iopub.status.idle":"2025-04-17T23:28:30.100939Z","shell.execute_reply.started":"2025-04-17T23:28:30.084684Z","shell.execute_reply":"2025-04-17T23:28:30.100382Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"['Genome-wide expression of the residual lung reacting to experimental Pneumonectomy.']"},"metadata":{}}],"execution_count":13},{"id":"fdbf8fd3-4b1c-48dc-a8d8-88125e89903e","cell_type":"code","source":"# Evaluate\nmetrics = evaluate(reference, pred)\nmetrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T23:28:30.101654Z","iopub.execute_input":"2025-04-17T23:28:30.101854Z","iopub.status.idle":"2025-04-17T23:28:31.822681Z","shell.execute_reply.started":"2025-04-17T23:28:30.101832Z","shell.execute_reply":"2025-04-17T23:28:31.822063Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6832cd5f8b2b424c9bb291001871741f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef3a061ef72744f796460dd292acd8c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dd383808da049db9367e4398e0ab237"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f44a817ef064c9185a3670fa4b6b863"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'BLEU': 0.0, 'ROUGE-2': 0.0714, 'ROUGE-L': 0.2}"},"metadata":{}}],"execution_count":14},{"id":"14a7428f-45f8-453b-872b-823ab3eba802","cell_type":"markdown","source":"## Evaluate Baseline Model","metadata":{}},{"id":"e42cf975-f42c-4819-bb58-65f9fe783a4a","cell_type":"code","source":"# Start time\nstart_time = datetime.now()\n\n# Run predictions\ny_pred = predict(tokenized_data['test']['text'], model, tokenizer)\ny_reference = tokenized_data['test']['titles']\n\n# End time\nend_time = datetime.now()\n\n# Calculate duration\ninference_duration = round((end_time - start_time).total_seconds() / 60, 2)\n\n# Evaluate\nmetrics = evaluate(y_reference, y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T23:28:31.897201Z","iopub.execute_input":"2025-04-17T23:28:31.897409Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nGenerating Titles:   7%|▋         | 10/138 [02:48<35:53, 16.83s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nGenerating Titles:  64%|██████▍   | 88/138 [24:21<13:41, 16.44s/it]","output_type":"stream"}],"execution_count":null},{"id":"9ed1ef7b-4640-4f86-be06-0b57c445611e","cell_type":"code","source":"# Experiment configuration\nfine_tuned = \"No\"\ncomments = \"Baseline Model\"\n\nexperiment_metadata = {\n    \"Start Time\": start_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n    \"End Time\": end_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n    \"Inference Duration (min)\": inference_duration,\n    \"Model\": model_path,\n    \"Tokenizer\": tokenizer.name_or_path,\n    \"Fine-Tuned\": fine_tuned,\n    \"Test Size\": len(y_reference),\n    \"Prompt Template\": prompt_template,\n    \"Comments\": comments\n}\n\n# Save the experiment\nsave_experiment(experiment_metadata, metrics, mode=\"overwrite\") # mode={overwrite, append, visualize}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c000fe06-7e86-4b84-8c38-070d35aac3bb","cell_type":"markdown","source":"## Model Fine-Tunning","metadata":{}},{"id":"b3bff379-a857-4347-aa85-cc169f0d4d88","cell_type":"code","source":"modules = find_all_linear_names(model)\nmodules","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:15:18.786978Z","iopub.execute_input":"2025-04-18T00:15:18.787266Z","iopub.status.idle":"2025-04-18T00:15:18.792776Z","shell.execute_reply.started":"2025-04-18T00:15:18.787244Z","shell.execute_reply":"2025-04-18T00:15:18.791987Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"['down_proj', 'v_proj', 'q_proj', 'o_proj', 'gate_proj', 'up_proj', 'k_proj']"},"metadata":{}}],"execution_count":22},{"id":"6eb75042-4d12-440a-8e52-96eaedf84094","cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=output_dir,                    # directory to save and repository id\n    num_train_epochs=1,                       # number of training epochs\n    per_device_train_batch_size=1,            # batch size per device during training\n    gradient_accumulation_steps=8,            # number of steps before performing a backward/update pass\n    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n    optim=\"paged_adamw_32bit\",\n    logging_steps=1,                         \n    learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n    max_steps=-1,\n    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n    group_by_length=False,\n    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n    report_to=\"wandb\",                  # report metrics to w&b\n    eval_strategy=\"steps\",              # save checkpoint every epoch\n    eval_steps = 0.2\n)\n\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=modules,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=tokenized_data['train'],\n    eval_dataset=tokenized_data['validation'],\n    peft_config=peft_config\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:42:28.919853Z","iopub.execute_input":"2025-04-18T00:42:28.920537Z","iopub.status.idle":"2025-04-18T00:42:30.810766Z","shell.execute_reply.started":"2025-04-18T00:42:28.920514Z","shell.execute_reply":"2025-04-18T00:42:30.809991Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/9022 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8625b33aa634f5495506512f5e7aa9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/2256 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d95fcc5f8dd14296a8c503d029fde51f"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":51},{"id":"a25bc9d0-05f4-4a3f-9579-0b9d057d5d2c","cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:42:34.630848Z","iopub.execute_input":"2025-04-18T00:42:34.631153Z","iopub.status.idle":"2025-04-18T04:43:31.133937Z","shell.execute_reply.started":"2025-04-18T00:42:34.631101Z","shell.execute_reply":"2025-04-18T04:43:31.133338Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1127' max='1127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1127/1127 4:00:45, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>226</td>\n      <td>1.638700</td>\n      <td>1.734248</td>\n    </tr>\n    <tr>\n      <td>452</td>\n      <td>1.575000</td>\n      <td>1.716754</td>\n    </tr>\n    <tr>\n      <td>678</td>\n      <td>1.636800</td>\n      <td>1.704379</td>\n    </tr>\n    <tr>\n      <td>904</td>\n      <td>1.536400</td>\n      <td>1.697459</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1127, training_loss=1.7208292630254005, metrics={'train_runtime': 14455.8057, 'train_samples_per_second': 0.624, 'train_steps_per_second': 0.078, 'total_flos': 4.038248661359002e+16, 'train_loss': 1.7208292630254005})"},"metadata":{}}],"execution_count":52},{"id":"ac4d558c-93bf-4d57-9435-0237e8eaead1","cell_type":"markdown","source":"## Evaluate Fine-Tuned Model ","metadata":{}},{"id":"3e9b9d22-35ed-49be-b983-deccc7bb1339","cell_type":"code","source":"# Start time\nstart_time = datetime.now()\n\n# Run predictions\ny_pred = predict(tokenized_data['test']['text'], model, tokenizer)\ny_reference = tokenized_data['test']['titles']\n\n# End time\nend_time = datetime.now()\n\n# Calculate duration\ninference_duration = round((end_time - start_time).total_seconds() / 60, 2)\n\n# Evaluate\nmetrics = evaluate(y_reference, y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T04:43:31.135478Z","iopub.execute_input":"2025-04-18T04:43:31.135735Z","iopub.status.idle":"2025-04-18T05:33:46.897836Z","shell.execute_reply.started":"2025-04-18T04:43:31.135718Z","shell.execute_reply":"2025-04-18T05:33:46.897174Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nGenerating Titles:   0%|          | 0/138 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\nDynamicCache + torch.export is tested on torch 2.6.0+ and may not work on earlier versions.\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\nGenerating Titles: 100%|██████████| 138/138 [50:14<00:00, 21.84s/it]\n","output_type":"stream"}],"execution_count":53},{"id":"3338c2cd-2b26-44f3-84c0-15d286851b8c","cell_type":"code","source":"# Experiment configuration\nfine_tuned = \"Yes\"\ncomments = \"Fine-tuned Model\"\n\nexperiment_metadata = {\n    \"Start Time\": start_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n    \"End Time\": end_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n    \"Inference Duration (min)\": inference_duration,\n    \"Model\": model_path,\n    \"Tokenizer\": tokenizer.name_or_path,\n    \"Fine-Tuned\": fine_tuned,\n    \"Test Size\": len(y_reference),\n    \"Prompt Template\": prompt_template,\n    \"Comments\": comments\n}\n\n# Save the experiment\nsave_experiment(experiment_metadata, metrics, mode=\"append\") # mode={overwrite, append, visualize}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T05:33:46.898753Z","iopub.execute_input":"2025-04-18T05:33:46.899024Z","iopub.status.idle":"2025-04-18T05:33:46.919858Z","shell.execute_reply.started":"2025-04-18T05:33:46.899001Z","shell.execute_reply":"2025-04-18T05:33:46.919170Z"}},"outputs":[{"name":"stdout","text":"Experiment saved to experiment_results.csv\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"            Start Time             End Time  Inference Duration (min)  \\\n0  2025-04-17 23:28:31  2025-04-18 00:06:13                     37.70   \n1  2025-04-18 00:32:57  2025-04-18 00:33:04                      0.11   \n2  2025-04-18 04:43:31  2025-04-18 05:33:45                     50.24   \n\n                              Model                         Tokenizer  \\\n0  meta-llama/Llama-3.2-3B-Instruct  meta-llama/Llama-3.2-3B-Instruct   \n1  meta-llama/Llama-3.2-3B-Instruct  meta-llama/Llama-3.2-3B-Instruct   \n2  meta-llama/Llama-3.2-3B-Instruct  meta-llama/Llama-3.2-3B-Instruct   \n\n  Fine-Tuned  Test Size                                    Prompt Template  \\\n0         No       1097  You are an expert at writing concise and infor...   \n1        Yes          2  You are an expert at writing concise and infor...   \n2        Yes       1097  You are an expert at writing concise and infor...   \n\n           Comments    BLEU  ROUGE-2  ROUGE-L  \n0    Baseline Model  0.0859   0.2321   0.3804  \n1  Fine-tuned Model  0.0000   0.3845   0.6009  \n2  Fine-tuned Model  0.1322   0.2575   0.4135  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Start Time</th>\n      <th>End Time</th>\n      <th>Inference Duration (min)</th>\n      <th>Model</th>\n      <th>Tokenizer</th>\n      <th>Fine-Tuned</th>\n      <th>Test Size</th>\n      <th>Prompt Template</th>\n      <th>Comments</th>\n      <th>BLEU</th>\n      <th>ROUGE-2</th>\n      <th>ROUGE-L</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2025-04-17 23:28:31</td>\n      <td>2025-04-18 00:06:13</td>\n      <td>37.70</td>\n      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n      <td>No</td>\n      <td>1097</td>\n      <td>You are an expert at writing concise and infor...</td>\n      <td>Baseline Model</td>\n      <td>0.0859</td>\n      <td>0.2321</td>\n      <td>0.3804</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2025-04-18 00:32:57</td>\n      <td>2025-04-18 00:33:04</td>\n      <td>0.11</td>\n      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n      <td>Yes</td>\n      <td>2</td>\n      <td>You are an expert at writing concise and infor...</td>\n      <td>Fine-tuned Model</td>\n      <td>0.0000</td>\n      <td>0.3845</td>\n      <td>0.6009</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2025-04-18 04:43:31</td>\n      <td>2025-04-18 05:33:45</td>\n      <td>50.24</td>\n      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n      <td>Yes</td>\n      <td>1097</td>\n      <td>You are an expert at writing concise and infor...</td>\n      <td>Fine-tuned Model</td>\n      <td>0.1322</td>\n      <td>0.2575</td>\n      <td>0.4135</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":54},{"id":"1675fe34-d801-481a-8a2c-b4eeb7956437","cell_type":"markdown","source":"## Close Work","metadata":{}},{"id":"7efd3193-95a9-44ef-8f18-b5138bcd40ef","cell_type":"markdown","source":"### Finish Weight & Bias","metadata":{}},{"id":"726b6a8d-352d-499c-bd8e-e0499c1db484","cell_type":"code","source":"wandb.finish(exit_code=0)\nmodel.config.use_cache = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T05:33:46.921519Z","iopub.execute_input":"2025-04-18T05:33:46.921715Z","iopub.status.idle":"2025-04-18T05:33:48.503661Z","shell.execute_reply.started":"2025-04-18T05:33:46.921698Z","shell.execute_reply":"2025-04-18T05:33:48.503164Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▆▅▅▁▁▁▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁▂▃▃▃████</td></tr><tr><td>eval/num_tokens</td><td>▁▁▁▁▁▃▄▆█</td></tr><tr><td>eval/runtime</td><td>▁▁▁▁▁████</td></tr><tr><td>eval/samples_per_second</td><td>███▇█▁▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>█████▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▄▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>█▄▂▃▂▂▁▂▂▂▂▃▄▂▂▁▂▂▂▁▁▂▂▁▂▂▃▃▂▃▂▄▄▂▂▂▂▃▃▂</td></tr><tr><td>train/learning_rate</td><td>▃▅▃█████▇▇▇▇▇▆▆▆▆▆▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▄▄▂▁▂▃▂▂▁▃▃▃▂▂▃▃▂▃▂▃▂▂▃▂▃▁▁▃▃▃▂▁▂▂▂▂▃▁</td></tr><tr><td>train/mean_token_accuracy</td><td>▁▅▄▅▆▄▃▄▄▅▆▆▆▇▄▆▃▄▆▅▆▅▅▄▅▄█▇▅▇▇▅▇▅▇▅▆▄▄▅</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.69746</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.63423</td></tr><tr><td>eval/num_tokens</td><td>1851392.0</td></tr><tr><td>eval/runtime</td><td>671.1725</td></tr><tr><td>eval/samples_per_second</td><td>3.361</td></tr><tr><td>eval/steps_per_second</td><td>0.42</td></tr><tr><td>total_flos</td><td>4.038248661359002e+16</td></tr><tr><td>train/epoch</td><td>0.99933</td></tr><tr><td>train/global_step</td><td>1127</td></tr><tr><td>train/grad_norm</td><td>0.15165</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6075</td></tr><tr><td>train/mean_token_accuracy</td><td>0.64069</td></tr><tr><td>train/num_tokens</td><td>2308096.0</td></tr><tr><td>train_loss</td><td>1.72083</td></tr><tr><td>train_runtime</td><td>14455.8057</td></tr><tr><td>train_samples_per_second</td><td>0.624</td></tr><tr><td>train_steps_per_second</td><td>0.078</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">magic-capybara-1</strong> at: <a href='https://wandb.ai/gabriel-ferreira-iowa-state-university/Fine-tune%20Llama-3.2-3B-Instruct%20for%20QTL%20Title%20Generation/runs/7p3bhtr5?apiKey=e9ae21b07ffe9cae402250cb116b0d8a66e9f8c6' target=\"_blank\">https://wandb.ai/gabriel-ferreira-iowa-state-university/Fine-tune%20Llama-3.2-3B-Instruct%20for%20QTL%20Title%20Generation/runs/7p3bhtr5?apiKey=e9ae21b07ffe9cae402250cb116b0d8a66e9f8c6</a><br> View project at: <a href='https://wandb.ai/gabriel-ferreira-iowa-state-university/Fine-tune%20Llama-3.2-3B-Instruct%20for%20QTL%20Title%20Generation?apiKey=e9ae21b07ffe9cae402250cb116b0d8a66e9f8c6' target=\"_blank\">https://wandb.ai/gabriel-ferreira-iowa-state-university/Fine-tune%20Llama-3.2-3B-Instruct%20for%20QTL%20Title%20Generation?apiKey=e9ae21b07ffe9cae402250cb116b0d8a66e9f8c6</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250418_002309-7p3bhtr5/logs</code>"},"metadata":{}}],"execution_count":55},{"id":"70aae45f-ca2f-44f6-b647-1acc7e25999e","cell_type":"markdown","source":"### Save Model Locally","metadata":{}},{"id":"d9ede4e2-fcce-4e2a-aae6-cc1d1f8aaf73","cell_type":"code","source":"# Save trained model and tokenizer\ntrainer.save_model(output_dir)\ntokenizer.save_pretrained(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T05:33:48.517378Z","iopub.execute_input":"2025-04-18T05:33:48.517590Z","iopub.status.idle":"2025-04-18T05:33:49.657280Z"}},"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"('llama-3.2-fine-tuned-model/tokenizer_config.json',\n 'llama-3.2-fine-tuned-model/special_tokens_map.json',\n 'llama-3.2-fine-tuned-model/tokenizer.json')"},"metadata":{}}],"execution_count":null},{"id":"873b9adb-f27f-4638-9e75-43ae8ff6d8ad","cell_type":"code","source":"# checkpoint = 'checkpoint-1125'\n\n# # Compress the checkpoint folder into a zip file.\n# shutil.make_archive(f'/kaggle/working/{output_dir}/{checkpoint}', 'zip', f'/kaggle/working/{output_dir}/{checkpoint}')\n\n# FileLink(f\"{output_dir}/{checkpoint}.zip\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"85ecf278-8272-4bf0-a19a-228db5099492","cell_type":"markdown","source":"### Push the model and tokenizer to the Hugging Face Hub","metadata":{}},{"id":"fbd337bb-96f7-4c1a-8e4e-5ce3a7f6e594","cell_type":"code","source":"model.push_to_hub(output_dir, use_temp_dir=False)\ntokenizer.push_to_hub(output_dir, use_temp_dir=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T05:37:02.637601Z","iopub.execute_input":"2025-04-18T05:37:02.638082Z","iopub.status.idle":"2025-04-18T05:38:42.480572Z","shell.execute_reply.started":"2025-04-18T05:37:02.638058Z","shell.execute_reply":"2025-04-18T05:38:42.479900Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f0223b5f793424c8c5b2780897af559"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d07d961519f4fd0b0d6a14dcf757f7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ec59889594e45f49a22510215db891b"}},"metadata":{}},{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Gabriel-Ferreira/llama-3.2-fine-tuned-model/commit/dfda0c6b3665eadf973a6013498754131970e6ba', commit_message='Upload tokenizer', commit_description='', oid='dfda0c6b3665eadf973a6013498754131970e6ba', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Gabriel-Ferreira/llama-3.2-fine-tuned-model', endpoint='https://huggingface.co', repo_type='model', repo_id='Gabriel-Ferreira/llama-3.2-fine-tuned-model'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":62}]}